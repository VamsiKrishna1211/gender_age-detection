{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    " \n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
    "    GlobalMaxPool2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, Conv2D\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize(filepath, input_shape=(224, 224)):\n",
    "    im = Image.open((filepath)).convert('RGB')\n",
    "    im = im.resize(input_shape)\n",
    "    im_array = np.array(im, dtype=\"uint8\")#[..., ::-1]\n",
    "    return np.array(im_array / (np.max(im_array)+ 0.001), dtype=\"float32\")\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.2,1.5],\n",
    "    zoom_range=0.2)\n",
    "\n",
    "def augment(im_array):\n",
    "    im_array = datagen.random_transform(im_array)\n",
    "    return im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(df, batch_size=8, aug=False):\n",
    "    df = df.sample(frac=1)\n",
    "\n",
    "    dict_age = {'(0, 2)' : 0,\n",
    "                '(3, 5)' : 1,\n",
    "                '(6, 10)' : 2,\n",
    "                '(11, 15)' : 3,\n",
    "                '(16, 20)' : 4,\n",
    "                '(21, 30)' : 5,\n",
    "                '(31, 40)' : 6,\n",
    "                '(41, 50)' : 7,\n",
    "                '(51, 60)' : 8,\n",
    "                '(61, 70)' : 9,\n",
    "                '(71, 80)' : 10,\n",
    "                 '(81, 90)' : 11,\n",
    "                 '(91, 100)' : 12}\n",
    "\n",
    "    while True:\n",
    "        for i, batch in enumerate([df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]):\n",
    "            if aug:\n",
    "                images = np.array([augment(read_and_resize(file_path)) for file_path in batch.path.values])\n",
    "            else:\n",
    "                images = np.array([read_and_resize(file_path) for file_path in batch.path.values])\n",
    "\n",
    "\n",
    "            #labels = np.array([dict_age[g] for g in batch.out_ages.values])\n",
    "            labels = np.array(batch.out_ages.values)\n",
    "\n",
    "            labels = labels[..., np.newaxis]\n",
    "\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizer,n_classes=1):\n",
    "\n",
    "    base_model = ResNet50(weights=\"./resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False)\n",
    "\n",
    "    #for layer in base_model.layers:\n",
    "    #    layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    #x = Flatten()\n",
    "    x = Dense(1000, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(750,activation=\"relu\")(x)\n",
    "    x = Dense(350,activation=\"relu\")(x)\n",
    "    x = Dense(100,activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    if n_classes == 1:\n",
    "        x = Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "    else:\n",
    "        x = Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    base_model = Model(base_model.input, x, name=\"base_model\")\n",
    "    if n_classes == 1:\n",
    "        base_model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=optimizer)\n",
    "    else:\n",
    "        base_model.compile(loss=\"sparse_categorical_crossentropy\", metrics=['acc'], optimizer=optimizer)\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(optimizer,n_classes=1):\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(5,5), input_shape=(256,256,3),padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#     model.add(GlobalAveragePooling2D())\n",
    "#     #model.add()\n",
    "#     #model.add(Dense(47150, activation = 'relu'))\n",
    "#     #model.add(Dense(47150/2, activation = 'relu'))\n",
    "#     #model.add(Dense(47150/4, activation = 'relu'))\n",
    "#     #model.add(Dense(47150/8, activation = 'relu'))\n",
    "#     #model.add(Dense(n_classes, activation = 'softmax'))\n",
    "        \n",
    "#     if n_classes == 1:\n",
    "#         model.add(Dense(n_classes, activation=\"sigmoid\"))#(x)\n",
    "#     else:\n",
    "#         model.add(Dense(n_classes, activation=\"softmax\"))#(x)\n",
    "#     #base_model = Model(base_model.input, x, name=\"base_model\")\n",
    "#     if n_classes == 1:\n",
    "#         model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=optimizer)\n",
    "#     else:\n",
    "#         model.compile(loss=\"sparse_categorical_crossentropy\", metrics=['acc'], optimizer=optimizer)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train and test CSV files \n",
      "Reading Done.\n",
      "Generating callback_list\n",
      "Done Generating callbacklist.\n",
      "generating Model\n",
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "/home/vamsik1211/ML/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating model\n",
      "Running Fit_generator\n",
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vamsik1211/ML/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "/home/vamsik1211/ML/lib/python3.7/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "499/500 [============================>.] - ETA: 0s - loss: 4.6079 - acc: 0.0125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsik1211/ML/lib/python3.7/site-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = \"./Dataset-copy/\"\n",
    "\n",
    "    dict_age = {'(0, 2)' : 0,\n",
    "                '(3, 5)' : 1,\n",
    "                '(6, 10)' : 2,\n",
    "                '(11, 15)' : 3,\n",
    "                '(16, 20)' : 4,\n",
    "                '(21, 30)' : 5,\n",
    "                '(31, 40)' : 6,\n",
    "                '(41, 50)' : 7,\n",
    "                '(51, 60)' : 8,\n",
    "                '(61, 70)' : 9,\n",
    "                '(71, 80)' : 10,\n",
    "                 '(81, 90)' : 11,\n",
    "                 '(91, 100)' : 12}\n",
    "\n",
    "    bag = 3\n",
    "\n",
    "    all_indexes = list(range(5))\n",
    "    \n",
    "    accuracies = []\n",
    "    print(\"Reading train and test CSV files \")\n",
    "    train_df = pd.read_csv(\"expanded_data_shuffled.csv\")\n",
    "    #test_df = pd.read_csv(\"test_gender_filtered_data_with_path.csv\")\n",
    "    tr_tr, tr_val = train_test_split(train_df, test_size=0.1)\n",
    "    tr_val['out_ages'].groupby\n",
    "    print(\"Reading Done.\")\n",
    "    cnt_ave = 0\n",
    "    predictions = 0\n",
    "#     print(\"Extracting test labels and test images from files\")\n",
    "#     test_images = load(\"imdb_test_images.npy\")\n",
    "#     test_labels = load(\"imdb_test_labels.npy\")\n",
    "#     print(\"Extracting Done.\")\n",
    "    #tr_tr, tr_val = train_test_split(train_df, test_size=0.1,random_state = 100)\n",
    "    file_path = \"imdb_age_recog_weights.h5\"\n",
    "    \n",
    "    print(\"Generating callback_list\")\n",
    "    \n",
    "#     log_dir=\"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    #early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                          mode=\"min\", \n",
    "                                          factor=0.1,\n",
    "                                          #cooldown=0,\n",
    "                                          patience=3,\n",
    "                                          verbose=1,\n",
    "                                          min_lr=0.00001)\n",
    "\n",
    "    callbacks_list = [checkpoint,\n",
    "                      reduce_on_plateau\n",
    "                      #tensorboard_callback\n",
    "                      #early\n",
    "                     ]  # early\n",
    "    \n",
    "    print(\"Done Generating callbacklist.\")\n",
    "    print(\"generating Model\")\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model = get_model( optimizer,n_classes=99)\n",
    "    print(\"Done generating model\")\n",
    "    print(\"Running Fit_generator\")\n",
    "    batch_size = 8\n",
    "    model.fit_generator(gen(tr_tr,batch_size=batch_size, aug=True), \n",
    "                        validation_data=gen(tr_val), \n",
    "                        epochs=200, \n",
    "                        verbose=1, \n",
    "                        workers=4,\n",
    "                        callbacks=callbacks_list,\n",
    "                        steps_per_epoch=500,#int(len(tr_tr)/batch_size),#int(10740.75), \n",
    "                        validation_steps=100,\n",
    "                        #validation_data=((test_images), test_labels)\n",
    "                        use_multiprocessing=True)\n",
    "    #model.save(file_path)\n",
    "    print(\"Trained Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"imdb_dataset.csv\")\n",
    "tr_tr, tr_val = train_test_split(train_df, test_size=0.1,random_state = 100)\n",
    "tr_val, tr_test = train_test_split(tr_val,test_size=0.1,random_state = 100)\n",
    "test_images = np.array([read_and_resize(file_path) for file_path in tr_test.path.values])\n",
    "test_labels = np.array([file_path for file_path in tr_test.path.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"imdb_test_images.npy\",test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array([int(g == \"m\") for g in tr_test.out_ages.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"imdb_test_labels.npy\",test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"imdb_age_recog_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "prob_max_class = max(y_predict[index])\n",
    "age_class = np.where(y_predict[index] == prob_max_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_class[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_age = {'(0, 2)' : 0,\n",
    "                '(3, 5)' : 1,\n",
    "                '(6, 10)' : 2,\n",
    "                '(11, 15)' : 3,\n",
    "                '(16, 20)' : 4,\n",
    "                '(21, 30)' : 5,\n",
    "                '(31, 40)' : 6,\n",
    "                '(41, 50)' : 7,\n",
    "                '(51, 60)' : 8,\n",
    "                '(61, 70)' : 9,\n",
    "                '(71, 80)' : 10,\n",
    "                 '(81, 90)' : 11,\n",
    "                 '(91, 100)' : 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_age[age_class[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
